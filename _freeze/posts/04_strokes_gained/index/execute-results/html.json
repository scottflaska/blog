{
  "hash": "f51aa54527ed323ebbdc73eadf118468",
  "result": {
    "markdown": "---\ntitle: \"Building a Strokes Gained Model for Golf\"\nauthor: \"Scott Flaska\"\ndate: \"2023-05-14\"\ndraft: false\ncategories: [shotlink, golf, r, machine learning]\nimage: \"images/predictions.png\"\nformat: \n  html:\n    toc: true\n    toc-location: left\n    toc-title: Contents\nexecute: \n  freeze: auto\n---\n\n\n# Introduction\n\nStrokes Gained is an interesting method for evaluating golfers, co-created by Columbia Business School professor Mark Broadie.\n\n\n```{=html}\n<blockquote class=\"twitter-tweet\" data-lang=\"en\" data-theme=\"dark\"><p lang=\"en\" dir=\"ltr\">Strokes Gained co-creator <a href=\"https://twitter.com/MarkBroadie?ref_src=twsrc%5Etfw\">@markbroadie</a> illustrates the statistic using Justin Thomas&#39; final-round eagle at the 2021 <a href=\"https://twitter.com/hashtag/THEPLAYERS?src=hash&amp;ref_src=twsrc%5Etfw\">#THEPLAYERS</a>. ðŸ”Ž <a href=\"https://t.co/LJ6qnp3ooE\">pic.twitter.com/LJ6qnp3ooE</a></p>&mdash; Golf Channel (@GolfChannel) <a href=\"https://twitter.com/GolfChannel/status/1633598893706338305?ref_src=twsrc%5Etfw\">March 8, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n```\n\nFrom my understanding, Strokes Gained is similar to Expected Points Added (EPA) in football - golfers are evaluated against an \"expected\" number of strokes remaining after each shot. This \"expected\" value is based off a predictive model trained on historical data[^1]. In this post, I'll build an *expected strokes remaining* model using PGA ShotLink data, which can later be used to estimate strokes gained. The model will use features in the shot-level data to predict how many strokes remaining the golfer has before the shot.\n\n[^1]: These methods have a major shortcoming in that they attribute the entire residual to a single golfer (or the teams involved). This could probably be corrected with a hierarchical/mixed model, but I'll save that for a future post.\n\n# Data Preparation\n\nFirst, I'll load the cleaned up data from my [previous post](https://scottflaska.github.io/blog/posts/02_shotlink_explore/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshot_level <- readRDS(\"../02_shotlink_explore/shot_level.rds\")\ncut_colors <- readRDS(\"../02_shotlink_explore/cut_colors.rds\")\n```\n:::\n\n\nBefore I start building a model, I want to gain a better understanding of how penalties, drops, and provisionals are handled in the data. The `shot_type_s_p_d` column has this information (S = Shot, P = Penalty, D = Drop, Pr = Provisional).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Filter to holes where player's had at least 1 penalty/drop/provisional\nshot_level %>% \n  mutate(is_p_d = ifelse(shot_type_s_p_d == 'S',0,1)) %>% \n  group_by(player,\n           round,\n           hole) %>% \n  mutate(p_d_count = sum(is_p_d)) %>% \n  filter(p_d_count > 0) %>% \n  ungroup() %>% \n  arrange(player,\n          round,\n          hole,\n          shot) %>% \n  select(player,\n         round,\n         hole,\n         shot,\n         type = shot_type_s_p_d,\n         strokes = num_of_strokes,\n         yards_out = yards_out_before_shot,\n         to_location) %>% \n  as.data.frame() %>% \n  head(14)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   player round hole shot type strokes    yards_out to_location\n1    2206     2   18    1    S       1 238.00000000       Rough\n2    2206     2   18    2    D       0  13.16666667       Rough\n3    2206     2   18    3    S       1  15.11111111       Green\n4    2206     2   18    4    S       1   1.16666667        Hole\n5    6527     4    6    1    S       1 222.00000000       Water\n6    6527     4    6    2    P       1  20.25000000       Other\n7    6527     4    6    3    D       0  20.25000000       Other\n8    6527     4    6    4    S       1  70.11111111       Green\n9    6527     4    6    5    S       1   0.02777778        Hole\n10   6567     1    6    1    S       1 215.00000000       Water\n11   6567     1    6    2    P       1  13.27777778       Other\n12   6567     1    6    3    D       0  13.27777778       Other\n13   6567     1    6    4    S       1  73.36111111       Green\n14   6567     1    6    5    S       1   3.66666667        Hole\n```\n:::\n:::\n\n\nReviewing the sample above, it looks like the `yards_out_before_shot` column (which will probably be the best predictor of strokes remaining) is a little misleading for penalty drops. For example, it looks like Player 6527 went in the water off the tee on the 6th Hole (Day 4), and had to take a penalty drop. The `yards_out_before_shot` value on the penalty and the drop is 20.25, but 70.11 on the first shot after the drop. This might be because ShotLink is measuring to where the ball landed in the water, but Player 6527 had to drop where they [entered the penalty area](https://www.usga.org/RulesFAQ/rules_answer.asp?FAQidx=210&Rule=0&Topic=4). For my model, I'll filter down to actual shots, where `shot_type_s_p_d = \"S\"`. I'll also add a `shot_id` so it will be easier to join back to the original data set later.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshot_level <- shot_level %>%\n  arrange(player,\n          round,\n          hole,\n          shot) %>% \n  mutate(shot_id = row_number())\n\nshots <- shot_level %>% \n  filter(shot_type_s_p_d == 'S')\n```\n:::\n\n\n# Linear Model\n\nNow I'll take a look at how distance from the hole correlates to strokes remaining.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshots %>% \n  ggplot(mapping = aes(x = yards_out_before_shot,\n                       y = strokes_remaining_before_shot)) +\n  geom_jitter(width = 0,\n              height = 0.1,\n              alpha = 0.25) +\n  geom_smooth(method = loess,\n              se = FALSE) +\n  scale_y_continuous(breaks = 1:10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Calculate R-Squared\ncor(shots$yards_out_before_shot, \n    shots$strokes_remaining_before_shot)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.682005\n```\n:::\n:::\n\n\nWhile there is a certainly a strong correlation between these numbers, the relationship is not quite linear. A log transformation should clean this up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshots <- shots %>% \n  mutate(log_yards_out_before_shot = log(yards_out_before_shot+1))\n  \n\nshots %>% \n  ggplot(mapping = aes(x = log_yards_out_before_shot,\n                       y = strokes_remaining_before_shot)) +\n  geom_jitter(width = 0,\n              height = 0.1,\n              alpha = 0.1) +\n  geom_smooth(method = loess,\n              se = FALSE) +\n  scale_y_continuous(breaks = 1:10)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Calculate R-Squared\ncor(shots$log_yards_out_before_shot, \n    shots$strokes_remaining_before_shot)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7876248\n```\n:::\n:::\n\n\nThe log transformation improves the R^2^ value, but it can probably be improved even further with a nonlinear model. To test this, I'll use cross-validation to evaluate out-of-sample performance. Ideally, I'd like to use some type of [time-series data splitting](https://topepo.github.io/caret/data-splitting.html#data-splitting-for-time-series) here to avoid any possible data leakage issues[^2], but I'll use a simpler method in this post. I'll split the 30 golfers into 10 groups of 3, and hold out one of these groups in the cross-validation process.\n\n[^2]: If I wanted to use this model to predict strokes remaining for future shot's, I would want to make sure I'm not using future shots when training/evaluating my model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayer_cv_groups <- shots %>% \n  select(player) %>% \n  unique() %>% \n  arrange(player) %>% \n  mutate(cv_group = cut_number(x = player, \n                               n = 10,\n                               labels = FALSE))\n\nshots <- shots %>% \n  inner_join(player_cv_groups,\n             by = \"player\")\n\nshots %>% \n  group_by(cv_group) %>% \n  summarize(golfers = n_distinct(player),\n            shots = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 3\n   cv_group golfers shots\n      <int>   <int> <int>\n 1        1       3   834\n 2        2       3   840\n 3        3       3   832\n 4        4       3   834\n 5        5       3   820\n 6        6       3   835\n 7        7       3   839\n 8        8       3   845\n 9        9       3   834\n10       10       3   835\n```\n:::\n\n```{.r .cell-code}\nfolds <- group_vfold_cv(data = shots,\n                        group = cv_group)\n```\n:::\n\n\nNow, using these folds, I'll find a performance baseline using the linear model above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njust_log_yards_recipe <- recipe(formula = strokes_remaining_before_shot ~\n                                  log_yards_out_before_shot, \n                                data = shots)\n\nlm_mod <- linear_reg(mode = \"regression\",\n                     engine = \"lm\")\n\nlm_workflow <- workflow() %>%\n  add_recipe(just_log_yards_recipe) %>%\n  add_model(lm_mod)\n\nlm_rs <- fit_resamples(object = lm_workflow,\n                       resamples = folds,\n                       control = control_resamples(save_pred = T))\n\nlm_rs %>%\n  collect_metrics() %>%\n  select(.metric,\n         mean) %>%\n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  .metric      mean\n1    rmse 0.5569516\n2     rsq 0.7900181\n```\n:::\n:::\n\n\nAs expected, the mean hold-out performance is very similar to the linear model fit on all the data.\n\n# XGBoost Model\n\nNext I'll try a gradient boosted tree model using the [XGBoost](https://xgboost.readthedocs.io/en/stable/) library. I love XGBoost. It trains and tunes relatively quickly, and you don't usually need to worry about other tedious pre-processing steps like centering, scaling, and imputing. Since XGBoost is nonlinear, I wont need to use the log transformation, and can switch back to just using `yards_out_before_shot`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njust_yards_recipe <- recipe(formula = strokes_remaining_before_shot ~\n                              yards_out_before_shot, \n                            data = shots)\n\nxgb_mod <- boost_tree(mode = \"regression\",\n                      engine = \"xgboost\")\n\nxgb_workflow <- workflow() %>%\n  add_recipe(just_yards_recipe) %>%\n  add_model(xgb_mod)\n\nxgb_rs <- fit_resamples(object = xgb_workflow,\n                        resamples = folds,\n                        control = control_resamples(save_pred = T))\n\nxgb_rs %>%\n  collect_metrics() %>%\n  select(.metric,\n         mean) %>%\n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  .metric      mean\n1    rmse 0.5236481\n2     rsq 0.8142939\n```\n:::\n:::\n\n\nThe XGBoost model improves performance without any parameter tuning, so hopefully I can get more out of it. First, I'll look at the out-of-sample predictions,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_preds <- xgb_rs %>% \n  collect_predictions() %>% \n  select(row_id = .row,\n         fold = id,\n         pred = .pred)\n\npreds_join <- shots %>% \n  as.data.frame() %>% \n  mutate(row_id = row_number()) %>% \n  inner_join(rs_preds,\n             by = \"row_id\") \n\npreds_join %>% \n  ggplot(mapping = aes(x = yards_out_before_shot,\n                       y = pred,\n                       color = fold)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n\nThe fits look good, but the nonlinear warts are showing. This model only uses one feature, `yards_out_before_shot`, but the predictions vary significantly at the same distance. For example, looking at the shots around 200 yards out, the predictions vary from 2.8ish to 3.5ish. This will cause confusion when we start attributing strokes gained to specific shots.\n\n# Monotonic Constraints\n\nLuckily, there is a fix - XGBoost has the ability to enforce [monotonic constraints](https://xgboost.readthedocs.io/en/stable/tutorials/monotonic.html), meaning I can force predicted strokes remaining to increase as yards out increases. I'll retrain my model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_mod <- boost_tree(mode = \"regression\") %>% \n  set_engine(engine = \"xgboost\",\n             monotone_constraints = c(1))\n\nxgb_workflow <- workflow() %>%\n  add_recipe(just_yards_recipe) %>%\n  add_model(xgb_mod)\n\nxgb_rs <- fit_resamples(object = xgb_workflow,\n                        resamples = folds,\n                        control = control_resamples(save_pred = T))\n\nxgb_rs %>%\n  collect_metrics() %>%\n  select(.metric,\n         mean) %>%\n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  .metric      mean\n1    rmse 0.5201534\n2     rsq 0.8168157\n```\n:::\n:::\n\n\nThe model performance actually improved slightly with the contraints, which is encouraging. Now I'll look at the out-of-sample predictions again.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_preds <- xgb_rs %>% \n  collect_predictions() %>% \n  select(row_id = .row,\n         fold = id,\n         pred = .pred)\n\npreds_join <- shots %>% \n  as.data.frame() %>% \n  mutate(row_id = row_number()) %>% \n  inner_join(rs_preds,\n             by = \"row_id\") \n\npreds_join %>% \n  ggplot(mapping = aes(x = yards_out_before_shot,\n                       y = pred,\n                       color = fold)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\nMuch better! Another interesting finding is that there is a \"jump\" in the predictions around 260 yards. The predicted strokes remaining jump from 3.2ish to 3.5ish. This could be because it's near the range where PGA Tour players can take more aggressive shots at the green to reduce their score. This is a good demonstration of the value of nonlinear models.\n\n# Ball Location Features\n\nNext, I'd like to add some more features. I'll start with the ball location (fairway, green, etc.). I cleaned up ball location data in [my last post](https://scottflaska.github.io/blog/posts/02_shotlink_explore/#:~:text=The%20columns%20seem%20to%20match%20up%20pretty%20well%2C%20but%20I%E2%80%99d%20like%20to%20consolidate%20them%20into%20more%20general%20to_location%20column.), but those were the `to_location` columns. Here I need the `from_location` columns because I am using `strokes_remaining_before_shot` and `yards_out_before_shot`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshots %>% \n  group_by(from_location_scorer,\n           from_location_laser) %>% \n  summarize(rows = n(),\n            .groups = \"keep\") %>% \n  arrange(desc(rows)) %>% \n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   from_location_scorer            from_location_laser rows\n1                 Green                        Unknown 3546\n2               Tee Box                                2162\n3               Fairway                   Left Fairway  550\n4               Fairway                  Right Fairway  533\n5         Primary Rough                     Left Rough  415\n6         Primary Rough                    Right Rough  375\n7    Intermediate Rough             Right Intermediate  144\n8    Intermediate Rough              Left Intermediate  112\n9        Fairway Bunker                        Unknown   92\n10    Green Side Bunker        Right Green Side Bunker   77\n11    Green Side Bunker  Right Front Green Side Bunker   66\n12    Green Side Bunker   Front Left Green Side Bunker   59\n13    Green Side Bunker         Left Green Side Bunker   47\n14    Green Side Bunker Front Center Green Side Bunker   34\n15               Fringe                   Left Fairway   28\n16                Other                        Unknown   20\n17        Primary Rough                        Unknown   20\n18               Fringe                  Right Fairway   18\n19          Native Area                     Left Rough   13\n20          Native Area                        Unknown   13\n21          Native Area                    Right Rough   11\n22    Green Side Bunker    Left Rear Green Side Bunker    3\n23                Other                     Left Rough    3\n24                Green                                   2\n25                Other                    Right Rough    2\n26                Water                        Unknown    2\n27    Green Side Bunker   Right Rear Green Side Bunker    1\n```\n:::\n\n```{.r .cell-code}\nshots <- shots %>% \nmutate(from_location = case_when(from_location_scorer == 'Green' \n                                 ~ 'Green',\n                                 from_location_scorer == 'Tee Box' \n                                 ~ 'Tee Box',\n                                 from_location_scorer %in% c('Fairway',\n                                                             'Fringe') \n                                 ~ 'Fairway',\n                                 from_location_scorer %in% c('Primary Rough',\n                                                             'Intermediate Rough') \n                                 ~ 'Rough',\n                                 from_location_scorer %in% c('Fairway Bunker',\n                                                             'Green Side Bunker') \n                                 ~ 'Bunker',\n                                 TRUE \n                                 ~ 'Other')) %>% \n  mutate(from_location = factor(from_location,\n                                ordered = T,\n                                levels = c(\"Green\",\n                                           \"Fairway\",\n                                           \"Tee Box\",\n                                           \"Rough\",\n                                           \"Bunker\",\n                                           \"Other\")))\n\nshots %>% \n  filter(player == 1810,\n         round == 1,\n         hole %in% 1:3) %>%\n  select(hole,\n         shot,\n         to_location,\n         from_location) %>% \n  arrange(hole,\n          shot) %>% \n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   hole shot to_location from_location\n1     1    1       Rough       Tee Box\n2     1    2       Green         Rough\n3     1    3       Green         Green\n4     1    4        Hole         Green\n5     2    1       Green       Tee Box\n6     2    2        Hole         Green\n7     3    1     Fairway       Tee Box\n8     3    2       Green       Fairway\n9     3    3       Green         Green\n10    3    4        Hole         Green\n```\n:::\n:::\n\n\nNow I'll train a new XGBoost model with this feature. Since I've updated the data set, I'll need to recreate the fold index and the recipe. I'll use [`step_dummy()`](https://recipes.tidymodels.org/reference/step_dummy.html) to convert `from_location` from a factor to binary terms for each location type.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfolds <- group_vfold_cv(data = shots,\n                        group = cv_group)\n\nwith_location_recipe <- recipe(formula = strokes_remaining_before_shot ~\n                                 yards_out_before_shot +\n                                 from_location,\n                               data = shots) %>% \n  step_dummy(from_location)\n\nxgb_mod <- boost_tree() %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"xgboost\",\n             monotone_constraints = c(1)) %>% \n  translate()\n\nxgb_workflow <- workflow() %>%\n  add_recipe(with_location_recipe) %>%\n  add_model(xgb_mod)\n\nxgb_rs <- fit_resamples(object = xgb_workflow,\n                        resamples = folds,\n                        control = control_resamples(save_pred = T))\n\nxgb_rs %>%\n  collect_metrics() %>%\n  select(.metric,\n         mean) %>%\n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  .metric      mean\n1    rmse 0.5124754\n2     rsq 0.8221943\n```\n:::\n:::\n\n\nNot much of an improvement, but the model got a little better.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrs_preds <- xgb_rs %>% \n  collect_predictions() %>% \n  select(row_id = .row,\n         fold = id,\n         pred = .pred)\n\npreds_join <- shots %>% \n  as.data.frame() %>% \n  mutate(row_id = row_number()) %>% \n  inner_join(rs_preds,\n             by = \"row_id\") \n\npreds_join %>% \n  ggplot(mapping = aes(x = yards_out_before_shot,\n                       y = pred,\n                       color = from_location)) +\n  geom_point() +\n  scale_color_manual(values = cut_colors) + \n  labs(title = \"Out-Of-Sample Predicted Strokes Remaining\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nThese new predictions make sense. Shots from the rough/bunker are harder and thus the predicted strokes remaining are higher. For example, at 100 yards, being in the rough (vs. fairway) increases expected strokes from 2.7ish to 3.0ish.\n\nAdding these features increased the prediction \"noise\" at each distance - similar to the results of the unconstrained XGBoost model. This stems from the variation in how each sub-model handles the new features, and it goes away if I filter down to a single hold out set (Fold 1).\n\n\n::: {.cell}\n\n```{.r .cell-code}\npreds_join %>% \n  filter(fold == 'Resample01') %>% \n  ggplot(mapping = aes(x = yards_out_before_shot,\n                       y = pred,\n                       color = from_location)) +\n  geom_point() +\n  scale_color_manual(values = cut_colors) + \n  labs(title = \"Out-Of-Sample Predicted Strokes Remaining\",\n       subtitle = \"Fold 1\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\nI could probably improve model performance with some parameter tuning, but I'll stop here for now.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}