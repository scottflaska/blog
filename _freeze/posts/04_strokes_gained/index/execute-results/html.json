{
  "hash": "2566d2ef653869c7cbc2c930635a58a0",
  "result": {
    "markdown": "---\ntitle: \"Building a Strokes Gained Model for Golf\"\nauthor: \"Scott Flaska\"\ndate: \"2023-05-01\"\ndraft: true\ncategories: [shotlink, golf, r, machine learning]\nimage: \"\"\nformat: html\n---\n\n\nStrokes Gained is an interesting method for evaluating golfers, co-created by Columbia Business School professor Mark Broadie.\n\n\n```{=html}\n<blockquote class=\"twitter-tweet\" data-lang=\"en\" data-theme=\"dark\"><p lang=\"en\" dir=\"ltr\">Strokes Gained co-creator <a href=\"https://twitter.com/MarkBroadie?ref_src=twsrc%5Etfw\">@markbroadie</a> illustrates the statistic using Justin Thomas&#39; final-round eagle at the 2021 <a href=\"https://twitter.com/hashtag/THEPLAYERS?src=hash&amp;ref_src=twsrc%5Etfw\">#THEPLAYERS</a>. ðŸ”Ž <a href=\"https://t.co/LJ6qnp3ooE\">pic.twitter.com/LJ6qnp3ooE</a></p>&mdash; Golf Channel (@GolfChannel) <a href=\"https://twitter.com/GolfChannel/status/1633598893706338305?ref_src=twsrc%5Etfw\">March 8, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n```\n\nFrom my understanding, Strokes Gained is similar to Expected Points Added (EPA) in football - golfers are evaluated against an \"expected\" number of strokes remaining after each shot. This \"expected\" value is based off a predictive model trained on historical data[^1]. In this post, I'll build my own Strokes Gained model using PGA ShotLink data. The model will use features in the shot-level data to predict how many strokes remaining the golfer has before the shot.\n\n[^1]: These methods have a major shortcoming in that they attribute the entire residual to a single golfer (or the teams involved). This could probably be corrected with a hierarchical/mixed model, but I'll save that for a future post.\n\nFirst, I'll load the cleaned up data from my [previous post](https://scottflaska.github.io/blog/posts/02_shotlink_explore/).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nshot_level <- readRDS(\"../02_shotlink_explore/shot_level.rds\")\ncut_colors <- readRDS(\"../02_shotlink_explore/cut_colors.rds\")\n```\n:::\n\n\nIntuitively, distance from the hole will probably be the best predictor of strokes remaining.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshot_level %>% \n  ggplot(mapping = aes(x = yards_out_before_shot,\n                       y = strokes_remaining_before_shot)) +\n  geom_jitter(width = 0,\n              height = 0.1,\n              alpha = 0.1) +\n  geom_smooth(method = loess,\n              se = FALSE) +\n  scale_y_continuous(breaks = 1:10) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Calculate R-Squared\ncor(shot_level$yards_out_before_shot, \n    shot_level$strokes_remaining_before_shot)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6659518\n```\n:::\n:::\n\n\nWhile there is a certainly a correlation between these numbers, the relationship is not quite linear. A log transformation should clean this up.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshot_level <- shot_level %>% \n  mutate(log_yards_out_before_shot = log(yards_out_before_shot+1))\n  \n\nshot_level %>% \n  ggplot(mapping = aes(x = log_yards_out_before_shot,\n                       y = strokes_remaining_before_shot)) +\n  geom_jitter(width = 0,\n              height = 0.1,\n              alpha = 0.1) +\n  geom_smooth(method = loess,\n              se = FALSE) +\n  scale_y_continuous(breaks = 1:10) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Calculate R-Squared\ncor(shot_level$log_yards_out_before_shot, \n    shot_level$strokes_remaining_before_shot)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.7783551\n```\n:::\n:::\n\n\nThe log transformation improves the R^2^ value, but it can probably be improved even further with a nonlinear model. To test this, I'll use cross-validation to evaluate out-of-sample performance. Ideally, I'd like to use some type of [time-series data splitting](https://topepo.github.io/caret/data-splitting.html#data-splitting-for-time-series) here to avoid any possible data leakage issues[^2], but I'll use a simpler method in this post. To avoid over-fitting, I need to make sure I don't include shot's from the same golfer-hole in the training and hold-out data[^3]. I'll split the 30 golfers into 10 groups of 3, and hold out one of these groups in the cross-validation process.\n\n[^2]: If I wanted to use this model to predict strokes remaining for future shot's, I would want to make sure I'm not using future shots when training/evaluating my model.\n\n[^3]: If a player took a triple bogey on a hole, the model could look at the other shots for the player on that hole and see that there is likely going to be a higher score.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayer_cv_groups <- shot_level %>% \n  select(player) %>% \n  unique() %>% \n  arrange(player) %>% \n  mutate(cv_group = cut_number(x = player, \n                               n = 10,\n                               labels = FALSE))\n\nshot_level <- shot_level %>% \n  inner_join(player_cv_groups,\n             by = \"player\")\n\nshot_level %>% \n  group_by(cv_group) %>% \n  summarize(golfers = n_distinct(player),\n            shots = n())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 10 Ã— 3\n   cv_group golfers shots\n      <int>   <int> <int>\n 1        1       3   837\n 2        2       3   850\n 3        3       3   843\n 4        4       3   838\n 5        5       3   822\n 6        6       3   837\n 7        7       3   848\n 8        8       3   862\n 9        9       3   841\n10       10       3   844\n```\n:::\n\n```{.r .cell-code}\nfolds <- group_vfold_cv(data = shot_level,\n                        group = cv_group)\n```\n:::\n\n\nNow, using these folds, I'll find a performance baseline using the linear model above.\n\n\n::: {.cell}\n\n```{.r .cell-code}\njust_log_yards_recipe <- recipe(formula = strokes_remaining_before_shot ~ log_yards_out_before_shot, \n                                data = shot_level)\n\nlm_mod <- linear_reg(mode = \"regression\",\n                     engine = \"lm\")\n\nlm_workflow <- workflow() %>%\n  add_recipe(just_log_yards_recipe) %>%\n  add_model(lm_mod)\n\nlm_rs <- fit_resamples(object = lm_workflow,\n                       resamples = folds)\n\nlm_rs %>%\n  collect_metrics() %>%\n  select(.metric,\n         mean) %>%\n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  .metric      mean\n1    rmse 0.5695931\n2     rsq 0.7817021\n```\n:::\n:::\n\n\nAs expected, the mean hold-out performance is very similar to the linear model fit on all the data.\n\nNext I'll try a gradient boosted tree model using the [xgboost](https://xgboost.readthedocs.io/en/stable/) library. I love xgboost. It trains and tunes relatively quickly, and you don't usually need to worry about other tedious pre-processing steps like centering, scaling, and imputing.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxgb_mod <- boost_tree(mode = \"regression\",\n                      engine = \"xgboost\")\n\nxgb_workflow <- workflow() %>%\n  add_recipe(just_log_yards_recipe) %>%\n  add_model(xgb_mod)\n\nxgb_rs <- fit_resamples(object = xgb_workflow,\n                        resamples = folds)\n\nxgb_rs %>%\n  collect_metrics() %>%\n  select(.metric,\n         mean) %>%\n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  .metric      mean\n1    rmse 0.5380285\n2     rsq 0.8052517\n```\n:::\n:::\n\n\nThe xgboost model improves performance without any parameter tuning, so I'll stick with it for now.\n\nNext, I'd like to add some more features. I'll start with the ball location (fairway, green, etc.). I cleaned up ball location data in [my last post](https://scottflaska.github.io/blog/posts/02_shotlink_explore/#:~:text=The%20columns%20seem%20to%20match%20up%20pretty%20well%2C%20but%20I%E2%80%99d%20like%20to%20consolidate%20them%20into%20more%20general%20to_location%20column.), but those were the `to_location` columns. Here I need the `from_location` columns because I am using `strokes_remaining_before_shot` and `yards_out_before_shot` . Rather than clean these columns, I'll use `dplyr::lag()` to grab the\n\n\n::: {.cell}\n\n```{.r .cell-code}\nshot_level %>% \n  filter(player == 1810,\n         round == 1,\n         hole == 1) %>% \n  select(shot,\n         to_location)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  shot to_location\n1    1       Rough\n2    2       Green\n3    3       Green\n4    4        Hole\n```\n:::\n\n```{.r .cell-code}\nshot_level <- shot_level %>% \n  group_by(player,\n           round,\n           hole) %>% \n  arrange(shot) %>% \n  mutate(from_location = lag(to_location)) %>% \n  ungroup() %>% \n  mutate(from_location = ifelse(shot == 1, 'Tee Box', from_location)) \n\nshot_level %>% \n  filter(player == 1810,\n         round == 1,\n         hole %in% 1:3) %>%\n  select(hole,\n         shot,\n         to_location,\n         from_location) %>% \n  arrange(hole,\n          shot) %>% \n  as.data.frame()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   hole shot to_location from_location\n1     1    1       Rough       Tee Box\n2     1    2       Green         Rough\n3     1    3       Green         Green\n4     1    4        Hole         Green\n5     2    1       Green       Tee Box\n6     2    2        Hole         Green\n7     3    1     Fairway       Tee Box\n8     3    2       Green       Fairway\n9     3    3       Green         Green\n10    3    4        Hole         Green\n```\n:::\n:::\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}